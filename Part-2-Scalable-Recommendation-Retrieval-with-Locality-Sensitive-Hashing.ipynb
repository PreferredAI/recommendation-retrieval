{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Recommendation Retrieval with Locality Sensitive Hashing\n",
    "\n",
    "From the previous example, we can see that the cost of exhaustive search is linear to the number of items, i.e., $n$ and number of features, i.e., $d$. \n",
    "\n",
    "In this part, we will practice to use Locality Sensitive Hashing to speed up the recommendation retrieval task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.lsh import *\n",
    "from utils.load_data import *\n",
    "from utils.pmf import *\n",
    "from utils.evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load train/test data and trained pmf model\n",
    "path_to_model = 'pmf_mvl1m.model'\n",
    "path_to_train_data = 'train_data'\n",
    "path_to_test_data  = 'test_data'\n",
    "\n",
    "pmf_mvl = pickle.load(open(path_to_model, 'rb'))\n",
    "train   = pickle.load(open(path_to_train_data, 'rb'))\n",
    "test    = pickle.load(open(path_to_test_data, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined below measure precision and recall value of the top-k recommendation list returned to each user by the model. The result is average over number of users in the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Linear Scanning Solution\n",
    "\n",
    "We first measure the precision@10 and recall@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measuring performance of the first 2000 users\n",
    "topK = 10\n",
    "data = pmf_mvl.w_Item\n",
    "queries = pmf_mvl.w_User[:2000,:]\n",
    "\n",
    "linear_prec, linear_recall = evaluate_topK(test, data, queries, topK)\n",
    "print('linear_prec@{0} \\t linear_recall@{0}'.format(topK))\n",
    "print('{0}\\t{1}'.format(linear_prec, linear_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing\n",
    "\n",
    "One of the most popular search protocal using Locality Sensitive Hashing structure is Hashtable look-up (illustrated below).  \n",
    "\n",
    "<img src=\"resources/images/lsh_retrieval.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances without post-processing\n",
    "\n",
    "In this experiment, we immediately build LSH index on the output of PMF algorithm. You should expect to see the precision and recall degeneration as compared to those of linear scanning solution. Here, we report three values here:\n",
    "\n",
    "1. relative_prec@10 = $\\frac{\\text{precision@10 of LSH Indexing}}{\\text{precision@10 of linear scanning}}$ \n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;  \n",
    "&nbsp;\n",
    "2. relative_rec@10    = $\\frac{\\text{recall@10 of LSH Indexing}}{\\text{recall@10 of linear scanning}}$\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;  \n",
    "&nbsp;\n",
    "3. touched = $\\frac{\\text{Average number of investigated items by LSH}}{\\text{Total number of items}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topK = 10\n",
    "b_vals = [4, 6, 8]\n",
    "L_vals = [5, 10]\n",
    "\n",
    "#queries = pmf_mvl.w_User\n",
    "#data    = pmf_mvl.w_Item\n",
    "\n",
    "print('#table\\t #bit \\t relative_prec@{0} \\t relative_recall@{0} \\t touched'.format(topK))\n",
    "for nt in L_vals:\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    for b in b_vals: \n",
    "        prec, recall, touched = evaluate_LSHTopK(test, data, -queries, CosineHashFamily, nt, b, dot, topK)\n",
    "        print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\".format(nt, b, prec/linear_prec, recall/linear_recall, touched)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances with post-processing Xbox transformation\n",
    "\n",
    "Now, before building LSH index, we first apply the Xbox transformation for both user and item vectors. This original maximum inner product search on the original representation becomes the maximum cosine similarity search on the new representation.\n",
    "\n",
    "\\begin{equation}\n",
    "P(y_i) = [y_i, \\sqrt{M^2 - ||y_i||^2}];  Q(x_u) = [x_u, 0]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply Xbox transformation\n",
    "topK = 10\n",
    "b_vals = [4, 6, 8]\n",
    "L_vals = [5, 10]\n",
    "\n",
    "\n",
    "M = np.linalg.norm(data, axis=1)\n",
    "max_norm = max(M)\n",
    "q_norm = np.sqrt((queries * queries).sum(axis=1))\n",
    "n_queries = queries/q_norm.reshape(queries.shape[0], 1)\n",
    "\n",
    "n_data = np.concatenate((data, np.sqrt(max_norm**2 - pow(M, 2)).reshape(data.shape[0], -1)), axis = 1)\n",
    "n_data = n_data/max_norm # normalized data vectors\n",
    "n_queries = np.concatenate((n_queries, np.zeros((n_queries.shape[0], 1))), axis = 1)\n",
    "\n",
    "print('Done X-box transformation!')\n",
    "print('#table\\t #bit \\t relative_prec@{0} \\t relative_recall@{0} \\t touched'.format(topK))\n",
    "for nt in L_vals:\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    for b in b_vals: \n",
    "        prec, recall, touched = evaluate_LSHTopK(test, n_data, -n_queries, CosineHashFamily, nt, b, dot, topK)\n",
    "        print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\".format(nt, b, prec/linear_prec, recall/linear_recall, touched)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
